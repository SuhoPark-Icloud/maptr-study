"""
`MapEncoder` í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ë©°, ì´ëŠ” MapTR ëª¨ë¸ì˜ ì¸ì½”ë” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.
ResNet50 ë°±ë³¸ê³¼ DepthNetì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ê¹Šì´ ë¶„í¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
"""

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import ResNet50_Weights, resnet50

_original_style_use = plt.style.use


def _patched_style_use(style):
    """
    `matplotlib.pyplot.style.use` í•¨ìˆ˜ì˜ íŒ¨ì¹˜ ë²„ì „ì…ë‹ˆë‹¤.
    ë ˆê±°ì‹œ seaborn ìŠ¤íƒ€ì¼ ì´ë¦„ì„ ì²˜ë¦¬í•˜ê³ , ìŠ¤íƒ€ì¼ ì ìš©ì„ ì‹œë„í•˜ë©°,
    ìŠ¤íƒ€ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ë„ ì˜¤ë¥˜ ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤.
    """
    if style == "seaborn-whitegrid":
        style = "seaborn-v0_8-whitegrid"
    try:
        _original_style_use(style)
    except OSError:
        pass


plt.style.use = _patched_style_use


class MapEncoder(nn.Module):
    def __init__(self, C=64, D=59):
        """
        `MapTR` ì¸ì½”ë”ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ResNet50 ë°±ë³¸ê³¼ DepthNetìœ¼ë¡œ êµ¬ì„±ë˜ë©°,
        ì¶œë ¥ íŠ¹ì§• ì±„ë„ ìˆ˜ (C)ì™€ ê¹Šì´ êµ¬ê°„ ìˆ˜ (D)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

        Args:
            C (int): ì¶œë ¥ íŠ¹ì§• ì±„ë„ ìˆ˜ (LSSë¡œ ì „ë‹¬ë  íŠ¹ì§•ì˜ ê°œìˆ˜).
            D (int): ê¹Šì´ êµ¬ê°„ ìˆ˜ (LSSì˜ ê¹Šì´ ë²”ìœ„ì— ì‚¬ìš©ë  ê°œìˆ˜).
        """
        super().__init__()
        self.C = C
        self.D = D

        # 1. Backbone (ResNet50)
        # Pre-trained ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

        # ìš°ë¦¬ëŠ” 'Layer 3' (Stride 16)ê¹Œì§€ì˜ íŠ¹ì§•ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # Layer 4 (Stride 32)ëŠ” ë§µí•‘ì— ì“°ê¸°ì— í•´ìƒë„ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤.
        self.backbone = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,
            resnet.layer3,
        )

        # ResNet Layer3ì˜ ì¶œë ¥ ì±„ë„ì€ 1024ê°œì…ë‹ˆë‹¤.
        backbone_dim = 1024

        # 2. DepthNet (1x1 Conv)
        # ì´ë¯¸ì§€ íŠ¹ì§•ì—ì„œ 'ê¹Šì´ ë¶„í¬(D)'ì™€ 'ì˜ë¯¸ íŠ¹ì§•(C)'ì„ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        # ì¶œë ¥ ì±„ë„: D (Depth) + C (Feature)
        self.depth_net = nn.Conv2d(
            backbone_dim, self.D + self.C, kernel_size=1, padding=0
        )

    def get_depth_feat(self, x):
        """
        ë°±ë³¸ê³¼ DepthNetì„ í†µí•´ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ ê¹Šì´ í™•ë¥ ê³¼ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            x (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B*N, 3, H, W].

        Returns:
            tuple:
                - depth (torch.Tensor): Softmax ì²˜ë¦¬ëœ ê¹Šì´ í™•ë¥  í…ì„œ [B*N, D, fH, fW].
                - feat (torch.Tensor): ì¶”ì¶œëœ ì´ë¯¸ì§€ íŠ¹ì§• í…ì„œ [B*N, C, fH, fW].
        """
        # 1. Backbone Forward
        # [B*N, 3, 450, 800] -> [B*N, 1024, 29, 50] (approx /16)
        x = self.backbone(x)

        # 2. DepthNet Forward
        # [B*N, 1024, fH, fW] -> [B*N, D+C, fH, fW]
        x = self.depth_net(x)

        # 3. Split into Depth and Feature
        # depth: ì•ìª½ Dê°œ ì±„ë„, feat: ë’¤ìª½ Cê°œ ì±„ë„
        depth = x[:, : self.D]
        feat = x[:, self.D :]

        # 4. Depth Softmax
        # ê¹Šì´ ê°’ì€ í™•ë¥ (Probability)ì´ì–´ì•¼ í•˜ë¯€ë¡œ Softmax ì ìš©
        depth = F.softmax(depth, dim=1)

        return depth, feat

    def forward(self, imgs):
        """
        `MapEncoder`ì˜ í¬ì›Œë“œ íŒ¨ìŠ¤.
        ì—¬ëŸ¬ ì¹´ë©”ë¼ì˜ ì´ë¯¸ì§€ ë°°ì¹˜(`imgs`)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ë°°ì¹˜ì™€ ì¹´ë©”ë¼ ì°¨ì›ì„ ê²°í•©í•œ í›„
        `get_depth_feat`ë¥¼ í†µí•´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ì´í›„ ì¶œë ¥(`depth`, `feat`)ì„ ë‹¤ì‹œ ë°°ì¹˜ ë° ì¹´ë©”ë¼ ì°¨ì›ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            imgs (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, N, 3, H, W].

        Returns:
            tuple:
                - depth (torch.Tensor): ê¹Šì´ í™•ë¥  í…ì„œ [B, N, D, fH, fW].
                - feat (torch.Tensor): ì´ë¯¸ì§€ íŠ¹ì§• í…ì„œ [B, N, C, fH, fW].
        """
        B, N, C_in, H, W = imgs.shape

        # Combine Batch and Camera dims for efficient processing
        imgs = imgs.view(B * N, C_in, H, W)

        # Encoder Forward
        depth, feat = self.get_depth_feat(imgs)

        # Reshape back to separate B and N
        # depth: [B*N, D, fH, fW] -> [B, N, D, fH, fW]
        # feat:  [B*N, C, fH, fW] -> [B, N, C, fH, fW]
        depth = depth.view(B, N, self.D, depth.shape[2], depth.shape[3])
        feat = feat.view(B, N, self.C, feat.shape[2], feat.shape[3])

        return depth, feat


# --- Testing Block ---
if __name__ == "__main__":
    print("ğŸ§ª Testing Map Encoder...")

    # Init
    encoder = MapEncoder(C=64, D=59)  # LSS ì„¤ì •ê³¼ ë§ì¶°ì•¼ í•¨ (1~60m)
    encoder.eval()  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ

    # Dummy Input (Dataset ì¶œë ¥ê³¼ ë™ì¼í•œ í˜•íƒœ)
    # Batch=1, Cam=6, Channel=3, Height=450, Width=800
    dummy_imgs = torch.randn(1, 6, 3, 450, 800)

    with torch.no_grad():
        depth, feat = encoder(dummy_imgs)

    print("âœ… Encoder Forward Success!")
    print(f"   Input Image: {dummy_imgs.shape}")
    print(f"   Output Depth: {depth.shape} (Expected: [1, 6, 59, 29, 50])")
    print(f"   Output Feat:  {feat.shape}  (Expected: [1, 6, 64, 29, 50])")

    # Check Softmax
    print(
        f"   Depth Sum Check: {depth[0, 0, :, 0, 0].sum().item():.4f} (Should be 1.0)"
    )
